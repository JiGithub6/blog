{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install a bunch of stuff. You'll need a virtual environment activated, or be running as root, otherwise your system will yell at you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests networkx matplotlib scipy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now let's import a bunch of stuff, and tell matplotlib that we want to see the graphs, instead of saving them to a file or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import requests\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and find the community following my Github. We'll ping the Github GraphQL using an OAuth Token that you can get by [following these directions](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/), and signing the [prerelease agreement](https://github.com/prerelease/agreement). I used Github's [GraphQL API Explorer](https://developer.github.com/v4/explorer/) to help format the queries. We'll get our followers, and everyone that they are following. You can see the queries [here](https://github.com/benhoff/blog/blob/master/content/github-analysis.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_graphql_url = 'https://api.github.com/graphql'\n",
    "\n",
    "# I'm using envirnomental variables so I can distribute this without leaking my token\n",
    "# a normal string will do\n",
    "oauth_token = os.getenv('GITHUB_OAUTH_TOKEN')\n",
    "\n",
    "# format strings in python 3.6+ are pretty neat\n",
    "headers={'Authorization': f'bearer {oauth_token}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "query = '''\n",
    "query {{\n",
    "viewer {{\n",
    "  login\n",
    "  followers ({follower_args})\n",
    "  {{\n",
    "    pageInfo {{\n",
    "      hasNextPage\n",
    "      endCursor\n",
    "    }}\n",
    "    nodes {{\n",
    "      login\n",
    "      following (first: 100) {{\n",
    "        pageInfo {{\n",
    "          hasNextPage\n",
    "          endCursor\n",
    "        }}\n",
    "        edges {{\n",
    "          node {{\n",
    "            login\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "}}\n",
    "'''\n",
    "\n",
    "user_query ='''\n",
    "query {{\n",
    "repositoryOwner(login: \"{login}\"){{\n",
    "  ... on User {{\n",
    "    following(first: 100 after: \"{cursor}\") {{\n",
    "      totalCount\n",
    "      pageInfo {{\n",
    "        endCursor\n",
    "        hasNextPage\n",
    "      }}\n",
    "      edges {{\n",
    "        node {{\n",
    "          login\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "}}\n",
    "'''\n",
    "\n",
    "initial_follower_args = 'first: 100'\n",
    "cursor_follower_args = \"first: 100 after: \\\"{cursor}\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to remember how to get the data out while I'm handeling the parsing of the data (which will be the majority of the code), so let's define helper methods to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(query):\n",
    "    # Use the `json` dumps method to translate the string into a dict\n",
    "    data = json.dumps({'query': query})\n",
    "    # We'll use requests, passing in the URL, query data, and header info\n",
    "    r = requests.post(github_graphql_url,\n",
    "                      data=data,\n",
    "                      headers=headers)\n",
    "\n",
    "    # Return the json data, stripping out the first key\n",
    "    return r.json()['data']\n",
    "\n",
    "# Define a helper method that deals with the cursor logic\n",
    "def get_follower_and_login_data(cursor=None):\n",
    "    # If there's a cursor, put it into our query\n",
    "    if cursor:\n",
    "        args = cursor_follower_args.format(cursor=cursor)\n",
    "    # Else, we'll just tell Github we want the first 100 followers\n",
    "    else:\n",
    "        args = initial_follower_args\n",
    "\n",
    "    # We need to put the args into our query\n",
    "    data_query = query.format(follower_args=args)\n",
    "\n",
    "    # Get the data\n",
    "    data = get_data(data_query)\n",
    "\n",
    "    # Strip out the follower data and our login name and return it\n",
    "    follower_data = data['viewer']['followers']\n",
    "    user_login_name = data['viewer']['login']\n",
    "    return follower_data, user_login_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a few methods to help us deal with the deeply nested JSON reply that we're going to get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers_and_next_page(my_follower_data: dict) -> (list, bool):\n",
    "    # who is following me?\n",
    "    my_followers = my_follower_data['nodes']\n",
    "    # do I have more than one page of followers?\n",
    "    has_next_page = my_follower_data['pageInfo']['hasNextPage']\n",
    "    # what is the cursor string that for use in the next query?\n",
    "    cursor = my_follower_data['pageInfo']['endCursor']\n",
    "    \n",
    "    return my_followers, has_next_page, cursor\n",
    "\n",
    "def get_first_layer_followers_and_next_page(a_follower: dict) -> (list,\n",
    "                                                                  bool):\n",
    "    \n",
    "    # Who is this user following?\n",
    "    following = a_follower['following']\n",
    "\n",
    "    # Is there a next page?\n",
    "    has_next_page = following['pageInfo']['hasNextPage']\n",
    "    # what is the cursor string that for use in the next query?\n",
    "    cursor = my_follower_data['pageInfo']['endCursor']\n",
    "    following = following['edges']\n",
    "    following = [x['node']['login'] for x in following]\n",
    "    \n",
    "    return following, has_next_page, cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using networkx to help us do the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the graph\n",
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get/parse the data and add it to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_follower_data, user_login_name = get_follower_and_login_data()\n",
    "graph.add_node(user_login_name)\n",
    "\n",
    "# Initial variable setup\n",
    "has_next_page = True\n",
    "on_last_page = False\n",
    "followers_with_many_following = {}\n",
    "\n",
    "# Continue to parse while there is a next page, or we're not on the\n",
    "# last page\n",
    "while has_next_page or not on_last_page:\n",
    "    results = get_followers_and_next_page(my_follower_data)\n",
    "    \n",
    "    my_followers = results[0]\n",
    "    has_next_page = results[1]\n",
    "    cursor = results[2]\n",
    "\n",
    "    # Add all of my followers to the graph, and capture the connection\n",
    "    for follower in my_followers:\n",
    "        follower_login = follower['login']\n",
    "        # Add them to our graphs\n",
    "        graph.add_node(follower_login)\n",
    "        graph.add_edge(follower_login, user_login_name)\n",
    "\n",
    "        # Now grab who they are following\n",
    "        results = get_first_layer_followers_and_next_page(follower)\n",
    "        \n",
    "        # Again, who our followers are following\n",
    "        following = results[0]\n",
    "        following_has_next_page = results[1]\n",
    "        following_cursor = results[2]\n",
    "\n",
    "        # Add all the people our followers are following\n",
    "        for login in following:\n",
    "            graph.add_node(login)\n",
    "            graph.add_edge(follower_login, login)\n",
    "            \n",
    "        # Some of the people we're following will have more than\n",
    "        # 100 followers, so save those\n",
    "        if following_has_next_page:\n",
    "            followers_with_many_following[follower_login] = following_cursor\n",
    "            \n",
    "    # Loop maintainence\n",
    "    if has_next_page == False:\n",
    "        on_last_page = True\n",
    "    else:\n",
    "        my_follower_data, _ = get_follower_and_login_data(cursor)\n",
    "   \n",
    "while followers_with_many_following:\n",
    "    keys = tuple(followers_with_many_following.keys())\n",
    "    for key in keys:\n",
    "        cursor = followers_with_many_following[key]\n",
    "        data = get_data(user_query.format(login=key, cursor=cursor))\n",
    "        follower_data = data['repositoryOwner']['following']\n",
    "        \n",
    "        if (not follower_data['pageInfo']['hasNextPage']\n",
    "            or follower_data['totalCount'] > 1000):\n",
    "            followers_with_many_following.pop(key)\n",
    "        else:\n",
    "            cursor = follower_data['pageInfo']['endCursor']\n",
    "            followers_with_many_following[key] = cursor\n",
    "            \n",
    "        logins = [x['node']['login'] for x in follower_data['edges']]\n",
    "        for login in logins:\n",
    "            graph.add_node(login)\n",
    "            graph.add_edge(key, login)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "# https://github.com/tpoisot/nxfa2\n",
    "def forceatlas2_layout(G, iterations=10, linlog=False, pos=None, nohubs=False,\n",
    "                       kr=0.001, k=None, dim=2):\n",
    "    \"\"\"\n",
    "    Options values are\n",
    "    g                The graph to layout\n",
    "    iterations       Number of iterations to do\n",
    "    linlog           Whether to use linear or log repulsion\n",
    "    random_init      Start with a random position\n",
    "                     If false, start with FR\n",
    "    avoidoverlap     Whether to avoid overlap of points\n",
    "    degreebased      Degree based repulsion\n",
    "    \"\"\"\n",
    "    # We add attributes to store the current and previous convergence speed\n",
    "    for n in G:\n",
    "        G.node[n]['prevcs'] = 0\n",
    "        G.node[n]['currcs'] = 0\n",
    "        # To numpy matrix\n",
    "    # This comes from the spares FR layout in nx\n",
    "    A = nx.to_scipy_sparse_matrix(G, dtype='f')\n",
    "    nnodes, _ = A.shape\n",
    "\n",
    "    try:\n",
    "        A = A.tolil()\n",
    "    except Exception as e:\n",
    "        A = (coo_matrix(A)).tolil()\n",
    "    if pos is None:\n",
    "        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n",
    "    else:\n",
    "        pos = pos.astype(A.dtype)\n",
    "    if k is None:\n",
    "        k = np.sqrt(1.0 / nnodes)\n",
    "        # Iterations\n",
    "    # the initial \"temperature\" is about .1 of domain area (=1x1)\n",
    "    # this is the largest step allowed in the dynamics.\n",
    "    t = 0.1\n",
    "    # simple cooling scheme.\n",
    "    # linearly step down by dt on each iteration so last iteration is size dt.\n",
    "    dt = t / float(iterations + 1)\n",
    "    displacement = np.zeros((dim, nnodes))\n",
    "    for iteration in range(iterations):\n",
    "        displacement *= 0\n",
    "        # loop over rows\n",
    "        for i in range(A.shape[0]):\n",
    "            # difference between this row's node position and all others\n",
    "            delta = (pos[i] - pos).T\n",
    "            # distance between points\n",
    "            distance = np.sqrt((delta ** 2).sum(axis=0))\n",
    "            # enforce minimum distance of 0.01\n",
    "            distance = np.where(distance < 0.01, 0.01, distance)\n",
    "            # the adjacency matrix row\n",
    "            Ai = np.asarray(A.getrowview(i).toarray())\n",
    "            # displacement \"force\"\n",
    "            Dist = k * k / distance ** 2\n",
    "            if nohubs:\n",
    "                Dist = Dist / float(Ai.sum(axis=1) + 1)\n",
    "            if linlog:\n",
    "                Dist = np.log(Dist + 1)\n",
    "            displacement[:, i] += \\\n",
    "                (delta * (Dist - Ai * distance / k)).sum(axis=1)\n",
    "            # update positions\n",
    "        length = np.sqrt((displacement ** 2).sum(axis=0))\n",
    "        length = np.where(length < 0.01, 0.01, length)\n",
    "        pos += (displacement * t / length).T\n",
    "        # cool temperature\n",
    "        t -= dt\n",
    "        # Return the layout\n",
    "    return dict(zip(G, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the alogrithm `Force Atlas 2` to deal with the spatialization of our Network Graph. The implementation is [here](https://github.com/tpoisot/nxfa2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = forceatlas2_layout(graph, linlog=False, nohubs=False, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph, positions, node_size=1, with_labels=False)\n",
    "plt.savefig('images/github-analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like my Github followers don't form much of a traditional social network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(graph))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
