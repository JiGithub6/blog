{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-api-python-client networkx scipy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import a bunch of stuff (most of it is to deal with oauth2) and have the graphs be displayed below when we print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import httplib2\n",
    "from apiclient.discovery import build\n",
    "\n",
    "from oauth2client.file import Storage\n",
    "from oauth2client.tools import run_flow\n",
    "from oauth2client.client import OAuth2WebServerFlow\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's API is stringly typed. And it's totally cool.\n",
    "\n",
    "I'm totally not irritated that these aren't enumerated in your python library.\n",
    "\n",
    "This is my not irritated voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YOUTUBE_READONLY_SCOPE = \"https://www.googleapis.com/auth/youtube.readonly\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google wants to store the response/counter response in a file so you don't have to reauthenticate. Which I get. But it wants to drop random files in your system, which I don't like. So we're going to hack around this by dropping them in the temporary directory, so that they'll be cleaned up on reboot. Technically using this API, I should manually delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = tempfile.mkstemp()\n",
    "os.close(filename[0])\n",
    "filename = filename[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need an OAuth2 credentials which you can get from [here](https://console.developers.google.com/) following [these instructions](https://developers.google.com/youtube/registering_an_application#Create_API_Keys). The `service` object, built using the method `build` (imported from `apiclient.discovery`) is the only object we'll be using for the rest. Everything else is just setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = '408396439838-o291hou2dra7l4i03vcg6ha6k7u8qk9b.apps.googleusercontent.com'\n",
    "# Just use a normal string. I'm doing this so I don't leak credentials.\n",
    "client_secret = os.getenv('YOUTUBE_CLIENT_SECRET')\n",
    "\n",
    "kwargs = {\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\n",
    "          \"token_uri\":\"https://accounts.google.com/o/oauth2/token\",\n",
    "          \"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "          \"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\", \"http://localhost\"]}\n",
    "\n",
    "flow = OAuth2WebServerFlow(client_id, client_secret, YOUTUBE_READONLY_SCOPE, **kwargs)\n",
    "\n",
    "storage = Storage(filename)\n",
    "credentials = storage.get()\n",
    "\n",
    "args = Namespace(auth_host_name='localhost',\n",
    "                 auth_host_port=[8080,8090],\n",
    "                 noauth_local_webserver=False,\n",
    "                 logging_level='ERROR')\n",
    "\n",
    "if credentials is None or credentials.invalid:\n",
    "    credentials = run_flow(flow, storage, args)\n",
    "\n",
    "service = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                http=credentials.authorize(httplib2.Http()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get every single subscriber and add the subscriber's name as a node, as well as capturing the connection. Store their channel id and name in the `stored_channel_ids` to get everyone that they are following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs ={'part': 'subscriberSnippet', 'mySubscribers': 'true', 'maxResults': '50'}\n",
    "\n",
    "# Get the snippet so we can get the publishedAt to do contagion analysis\n",
    "# kwargs ={'part': 'subscriberSnippet,snippet', 'mySubscribers': 'true', 'maxResults': '50'}\n",
    "\n",
    "has_next_page = True\n",
    "stored_channel_ids = {}\n",
    "\n",
    "# NOTE: change this to be your channel name.\n",
    "channel_name = 'benhoff'\n",
    "graph.add_node(channel_name)\n",
    "\n",
    "while has_next_page:\n",
    "    results = service.subscriptions().list(**kwargs).execute()\n",
    "    has_next_page = results.get('nextPageToken')\n",
    "    if has_next_page is not None:\n",
    "        kwargs['pageToken'] = has_next_page\n",
    "        \n",
    "    for result in results['items']:\n",
    "        name = result['subscriberSnippet']['title']\n",
    "        channel_id = result['subscriberSnippet']['channelId']\n",
    "        stored_channel_ids[channel_id] = [name,]\n",
    "        # if ever interested in doing contagion type analysis\n",
    "        # result['snippet]['publishedAt']\n",
    "        graph.add_node(name)\n",
    "        graph.add_edge(name, channel_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterate through the `stored_channel_ids` to get every channel that our followers are following. Add those to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs ={'part': 'snippet', 'channelId': '', 'maxResults': '50'}\n",
    "while stored_channel_ids:\n",
    "    channel_ids = tuple(stored_channel_ids.keys())\n",
    "    \n",
    "    for channel_id in channel_ids:\n",
    "        # update the query to include this channel id\n",
    "        kwargs['channelId'] = channel_id\n",
    "        # update the page token, if we have one\n",
    "        try:\n",
    "            page_token = stored_channel_ids[channel_id][1]\n",
    "            kwargs['pageToken'] = page_token\n",
    "        # pop the `pageToken` out of kwargs, if we don't have one\n",
    "        except IndexError:\n",
    "            try:\n",
    "                kwargs.pop('pageToken')\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        results = service.subscriptions().list(**kwargs).execute()\n",
    "        \n",
    "        if results.get('error'):\n",
    "            stored_channel_ids.pop(channel_id)\n",
    "            continue\n",
    "            \n",
    "        name = stored_channel_ids[channel_id][0]\n",
    "        for result in results['items']:\n",
    "            follower_name = result['snippet']['title']\n",
    "            graph.add_node(follower_name)\n",
    "            graph.add_edge(name, follower_name)\n",
    "            \n",
    "        has_token = results.get('nextPageToken')\n",
    "\n",
    "        if not has_token:\n",
    "            # remove this from the dict\n",
    "            stored_channel_ids.pop(channel_id)\n",
    "        else:\n",
    "            try:\n",
    "                stored_channel_ids[channel_id][1] = has_token\n",
    "            except IndexError:\n",
    "                stored_channel_ids[channel_id].append(has_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "# https://github.com/tpoisot/nxfa2\n",
    "def forceatlas2_layout(G, iterations=10, linlog=False, pos=None, nohubs=False,\n",
    "                       kr=0.001, k=None, dim=2):\n",
    "    \"\"\"\n",
    "    Options values are\n",
    "    g                The graph to layout\n",
    "    iterations       Number of iterations to do\n",
    "    linlog           Whether to use linear or log repulsion\n",
    "    random_init      Start with a random position\n",
    "                     If false, start with FR\n",
    "    avoidoverlap     Whether to avoid overlap of points\n",
    "    degreebased      Degree based repulsion\n",
    "    \"\"\"\n",
    "    # We add attributes to store the current and previous convergence speed\n",
    "    for n in G:\n",
    "        G.node[n]['prevcs'] = 0\n",
    "        G.node[n]['currcs'] = 0\n",
    "        # To numpy matrix\n",
    "    # This comes from the spares FR layout in nx\n",
    "    A = nx.to_scipy_sparse_matrix(G, dtype='f')\n",
    "    nnodes, _ = A.shape\n",
    "\n",
    "    try:\n",
    "        A = A.tolil()\n",
    "    except Exception as e:\n",
    "        A = (coo_matrix(A)).tolil()\n",
    "    if pos is None:\n",
    "        pos = np.asarray(np.random.random((nnodes, dim)), dtype=A.dtype)\n",
    "    else:\n",
    "        pos = pos.astype(A.dtype)\n",
    "    if k is None:\n",
    "        k = np.sqrt(1.0 / nnodes)\n",
    "        # Iterations\n",
    "    # the initial \"temperature\" is about .1 of domain area (=1x1)\n",
    "    # this is the largest step allowed in the dynamics.\n",
    "    t = 0.1\n",
    "    # simple cooling scheme.\n",
    "    # linearly step down by dt on each iteration so last iteration is size dt.\n",
    "    dt = t / float(iterations + 1)\n",
    "    displacement = np.zeros((dim, nnodes))\n",
    "    for iteration in range(iterations):\n",
    "        displacement *= 0\n",
    "        # loop over rows\n",
    "        for i in range(A.shape[0]):\n",
    "            # difference between this row's node position and all others\n",
    "            delta = (pos[i] - pos).T\n",
    "            # distance between points\n",
    "            distance = np.sqrt((delta ** 2).sum(axis=0))\n",
    "            # enforce minimum distance of 0.01\n",
    "            distance = np.where(distance < 0.01, 0.01, distance)\n",
    "            # the adjacency matrix row\n",
    "            Ai = np.asarray(A.getrowview(i).toarray())\n",
    "            # displacement \"force\"\n",
    "            Dist = k * k / distance ** 2\n",
    "            if nohubs:\n",
    "                Dist = Dist / float(Ai.sum(axis=1) + 1)\n",
    "            if linlog:\n",
    "                Dist = np.log(Dist + 1)\n",
    "            displacement[:, i] += \\\n",
    "                (delta * (Dist - Ai * distance / k)).sum(axis=1)\n",
    "            # update positions\n",
    "        length = np.sqrt((displacement ** 2).sum(axis=0))\n",
    "        length = np.where(length < 0.01, 0.01, length)\n",
    "        pos += (displacement * t / length).T\n",
    "        # cool temperature\n",
    "        t -= dt\n",
    "        # Return the layout\n",
    "    return dict(zip(G, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to position all our nodes and edges so that we can make some sense of it. So we'll use either the forceatlas2 method, or the spring method that comes with networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore\n",
    "positions = forceatlas2_layout(graph, linlog=False, nohubs=False, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = nx.spring_layout(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph, spring, node_size=1, with_labels=False)\n",
    "# plt.savefig('images/github-analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(graph))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
